Name: Salman Ahmad 
Email: saahmad@stanford.edu

Name: Andrew Hershberger
Email: adh15@stanford.edu



=== Bayesian Network Knowledge Engineering ===

== Question 1 ==

Observing work history as 'Stable' makes the marginal of Credit Worthiness
47.02% (Positive) and 52.98% (Negative)

Observing work history as 'Unstable' makes the marginal of Credit Worthiness
39.21% (Positive) and 60.88% (Negative)

The change does NOT change the marginal for Payment History. It is expected
because Work History and Payment History are part of a V-Structure and since
Reliability is not observed the influence cannot flow from Work History to
Payment History.

== Question 2 ==

When Reliability is set to 'Reliable' the marginal for Credit Worthiness is
57.91% (Positive) and 42.09% (Negative).

When Reliability is set to 'Unreliable' the marginal for Credit Worthiness is
27.10% (Positive) and 72.90% (Negative).

== Question 3 ==

Change Work History no longer impacts the marginal for Credit Worthiness. This
is expected because there is no active trail between Work History and Credit
Worthiness in this case. The only trail (Work History - Reliability - Credit
Worthiness) is "blocked" off because Reliability is observed.

== Question 4 ==

Yes, the marginal for Payment History does change. This is expected because
Work History -> Reliability <- Payment History form a V-Structure. Since
reliability is "observed" the V-structure is "activated" and influence can
flow between Work History and Payment History.

== Question 5 ==

It would appear that the Future Income CPD has been tampered with. When Worth
is set to High the marginal in the Future income is biased towards "Not
Promising" and when Worth is set to Low the Future Income is biased towards
Promising. This seems to be the opposite of what you would expect - People who
are high worth now will probably be able to continue to have more money in the
future.

== Question 6 ==

I swapped the two rows in the Future Income CPD. So, for every columns, I put
the "Promising" value as "Not Promising" and vice versa. Everything seems to
work now. When I change assets to Wealthy, the Credit Worthiness increases.

== Question 7 ==

I made Education a parent of Profession and a Child of Age, as I thought that
a person's age may indicate what their highest level of education they have
(currently) attained. I also made Education a parent of profession as I
thought that person's education will impact their professional status.0.05

The CPD for Education is Rows = Education (high school, college, graduate) and
Columns = Age (a16_21, a22_21, a66_up)

0.8 0.6 0.6 
0.19 0.3 0.3 
0.01 0.1 0.1

The CPD for Profession is: Rows = Profession (High income, medium income, low
income) Columns = Education (high school, college, graduate)

0.05 0.3 0.35 
0.15 0.5 0.4 
0.8 0.2 0.25

When I observe education to high school I notice that credit worthiness is
decreases relative to college and graduate education. As an added bonus,
people with graduate educations actually result in a lower credit worthiness
marginal because life as a graduate student sucks so why should the real world
be any different (*sigh*).

== Question 8 ==

I added a married node that indicates if a person is married or single (I do
not model divorced). I attach this as a child of Age and a Parent of
Reliability because I would guess that married people are considered more
"responsible" and less likely to take risks that would impact their
reliability.

The CPD for married is: (Rows are married {Married, Single} and Columns are
Are {a16_22, a22_65, a65_up}). Notice how the probability of married people
goes down with a66_up because I factor in the sad fact that people die.

0.1 0.8 0.6 
0.9 0.2 0.4

When I switch the the Married variable to Married I notice that the Credit
Worthiness increase from Single. Also, interestingly enough, when I fix age to
16-22 the different is not that much. This I suppose makes sense as well, you
should not penalize a young person for not being married yet and the network
accounts for this.





=== Scene Segmentation â€“ Markov Nets ===




== Question 1 ==

See source code

== Question 2 == 

See source code

(a)

We ran the experiment on the pixel level with four different values for
potts_lambda - 0.1, 0.3, 0.5 and 0.9. The accuracy results are shown below:

	0.1 - 0.74638
	0.3 - 0.76266
	0.5 - 0.77733
	0.9 - 0.79766
	1.0 - 0.80142
	2.0 - 0.82862

From the results we got back, it seemed that for Pixel level data, higher
Potts factors performed better. Qualitatively, the high Potts factors resulted
in images that had less "holes" in them which caused the labelings to be more
contiguous.

(b)

Overall, when we use superpixels we get better performance. We fixed
potts_lambda to 0.1 and rans it SPum = 1, 2, 3, 4. The results are shown
below:

	1 - 0.80558 
	2 - 0.80713
	3 - 0.81372 
	4 - 0.80811

Interestingly enough, at SPnum of 4, we get slightly less performance than
SPnum of 3. When we changed the potts_lambda value from 0.1 to 0.001 we did
not observe a huge different in performance. For example, SPnum 4 went from
0.80811 to 0.79926 and when we used potts_lambda of 0.5 at SPnum = 4 became
0.8533

Qualitatively, we noticed that the images we got back were much more "jaggy"
than with the pixel-level runs.

== Question 3 ==

See source code

(a)

Potts = 	.1
Contrast = 	0.0001
Accuracy = 0.74439 (vs 0.74638)


Potts = 	.1
Contrast = 	0.001
Accuracy =  0.74168 (vs 0.74638)

Potts = 	.1
Contrast = 	0.001
Accuracy =  0.74168 (vs 0.74638)

Potts = 	.1
Contrast = 	0.1
Accuracy =  0.73589 (vs 0.74638)


It actually seems that adding the contrast values reduces performance at the
fixed Potts weight of 0.1. I suppose this kind of make makes sense because I
would imagine that when you look at individual pixel, the contrast between
them will, on average be smaller because adjacent pixels are often very
similar. This would mean that the energy function will be smaller (because of
the negative exponent on the e) and thus may result in poorer performance.

In fact, the average contrast between pixels was 2.1315e+03 while the average
contrast between super pixels (found in the next section) was 1.0624e+04

(b)

We ran several experiments. The results are shown below. The accuracy should
be compared against a baseline of just Potts of 0.80713. The same value of
Potts was used in all runs (0.1)

	SPNum = 	2
	contrast = 	0.0001
	accuracy =	0.81199

	SPNum = 	2
	contrast = 	0.00001
	accuracy =	0.80729

	SPNum = 	2
	contrast = 	0.01
	accuracy =	0.80935

	SPNum = 	2
	contrast = 	0.0005
	accuracy =	0.81159

	SPNum = 	2
	contrast = 	0.01
	accuracy =	0.80935

	SPNum = 	2
	contrast = 	0.1
	accuracy =	0.80842

	SPNum = 	2
	contrast = 	0.5
	accuracy =	0.80737

	SPNum = 2 
	contrast = 1.0 
	accuracy = 0.8076

The contract model definitely seems to help the segmentation. In the worst
case it had an accuracy of 0.80737 which was still better than without
contrast at all. Interestingly enough, it seemed that smaller contrast values
perform better than larger ones. For instance, 0.0001 does 0.81199 while 0.5
does 0.80737. Then again, too small values do poorly as well (0.0001 did
0.81199).


== Question 4 ==

You can see the source code for the complete set of energies that I added.

Initially I went overboard and added a bunch of things that made semantic
sense to me, but these actually made the accuracy worse. Things like grass
goes below foreground and tress are above grass. Additionally, I initially
made sure that water was under everything as it made sense. It would be
difficult for there to be a road or a tree that was under water. These results
caused a huge decrease. I then removed everything and added only selected
items that I observed from the output images needed tweaking.

I added the sky to be above the road and the foreground. This actually did not
help at all, which is not surprising, because the algorithm gets the sky
really well and thus, adding energies would probably not help very much.

I then noticed that the trees were slightly off in the image and made the
trees above the road and foreground as well. This improved the results.

The biggest addition I made was making the buildings above the road. I noticed
this because there was a bunch of "red" that seeped into the purple road
labels and added this energy to fix that.

Lastly, I noticed that there was a big blob of "water" and "mountain" in the
middle of road. This was obviously a misclassification but I could not get it
to go away. So, I cheated a bit. I added energies to make the mountain and
water above the road, the foreground, and even the sky! Even though this makes
no semantic sense, I wanted to force the algorithm to not make these label
assignments. This approach worked. The mountain labels all disappeared and the
water labels got smaller. Obviously, this would not extend to other images.

To evaluate the performance of the energies, I used the highest accuracy
experiment we had so far - using SPNum = 2, a Potts factor of 0.1 and a
Contrast factor of 0.001. The results before and after the energies were added
is shown below:

	After = 0.82857 
	Before = 0.81199

It would seem that my energies did in fact improve the results. The trick is
adding the correct energies to correct for the mistakes that the algorithm is
currently making. There was a weird "water" line that the algorithm was
detecting instead of the curb side. I tried to tweak the energies to correct
this but I was unable to do so. I would imagine that the contrast it has is
just too large and the energies cannot account for it.

=== Object-superpixel interactions ===


== Question 1 ==

See source code.

(a)

	SPNum = 2
	Potts = 0.1
	Contrast = 0.001
	Full = 0

To test the results I used the settings above and varied the object weights. 

	Object = 0.25
	Accuracy = 0.81183
	Observation = Only one one bounding box not in the right place

	Object = 0.1
	Accuracy = 0.81188
	Observation = Many boxes now

	Object = 0.5
	Accuracy = 0.81145
	Observation = Only one boxes.

	Object = 0.75
	Accuracy = 0.81145
	Observation = Only one box

	Object = 0.05
	Accuracy = 0.81183
	Observation = Many more boxes now

It would seem that smaller object weights result in more detections. In
particular, with an object weight of 0.25 there was only one bounding box
while an object weight of 0.1 results in around 5-10 detections. This trend
continues with other values of object weight. At a very small weight of 0.05
there were even more detections but many of them were obviously wrong. This
makes sense, because of the energy functions. The Potts model tries to
minimize the energy as described in page 127 in the text. Having a higher
object weight will result in a higher energy and thus makes it less likely
that the object will be detected. At very high object weights this means that
there will be a lot of false negatives and at very low object weights this
means that there will be a lot of false positives - which is exactly what we
observed.

== Question2 ==

TODO